{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6wn4Wq13yiK"
   },
   "source": [
    "**Data** **Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import emoji\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GAT/Mental-Health-Twitter.csv\")\n",
    "\n",
    "# Make sure the labels are balanced\n",
    "positive_samples = df[df['label'] == 1].sample(n=1000, random_state=42)\n",
    "negative_samples = df[df['label'] == 0].sample(n=1000, random_state=42)\n",
    "\n",
    "# Combine and shuffle\n",
    "reduced_df = pd.concat([positive_samples, negative_samples]).sample(frac=1, random_state=42)\n",
    "\n",
    "# Save to new CSV\n",
    "reduced_df.to_csv(\"reduced_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/GAT/reduced_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head().to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Columns:\", df.columns)\n",
    "if 'label' in df.columns and 'post_text' in df.columns:\n",
    "    df = df.rename(columns={'post_text': 'text'})\n",
    "    df = df[['text', 'label']]\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "print(df.head().to_string(index=False))\n",
    "\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data cleaning(text)\n",
    "def clean_text(text):\n",
    "  text = re.sub(r'@\\w+|#\\w+|http\\S+|www\\S+', '', text) ## removing symols\n",
    "  text = emoji.replace_emoji(text, replace='') ## removing emojis\n",
    "  text = contractions.fix(text) ## expand contractions\n",
    "  text = text.translate(str.maketrans('', '', string.punctuation)) ## femove punctuations\n",
    "  text = text.lower()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "#stop_words\n",
    "\n",
    "def preprocess(text):\n",
    "  tokens = word_tokenize(text) # tokernizer\n",
    "  tokens = [word for word in tokens if word not in stop_words] # remove stop words\n",
    "  tokens = [stemmer.stem(word) for word in tokens] # stemming\n",
    "  return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "df['processed_text'] = df['cleaned_text'].apply(preprocess)\n",
    "df = df[['processed_text', 'label']]\n",
    "df.columns = ['text', 'label']\n",
    "print(df.head().to_string())\n",
    "df.to_csv('Preprocessed_Mental_Health_Tweets.csv', index=False) #saving preprocessed data(incase)\n",
    "print(len(df))\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fxfu-ep4DRg"
   },
   "source": [
    "**Embedding** **Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import DebertaTokenizer, DebertaModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pretrained models and tokenizer\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "roberta_model = RobertaModel.from_pretrained('roberta-base')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "deberta_model = DebertaModel.from_pretrained('microsoft/deberta-base')\n",
    "deberta_tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting models to evaluation mode\n",
    "bert_model.eval()\n",
    "roberta_model.eval()\n",
    "deberta_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device)\n",
    "roberta_model.to(device)\n",
    "deberta_model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Preprocessed_Mental_Health_Tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "\n",
    "def get_embedding(text, tokenizer, model):\n",
    "  if not isinstance(text, str):\n",
    "    text = str(text)\n",
    "  inputs = tokenizer(text, return_tensors='pt', max_length=MAX_LEN, truncation=True, padding='max_length') #preparing text for model\n",
    "  inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "  with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden = outputs.last_hidden_state #shape : (1, max_len, hidden_size)\n",
    "    avg_pooled = last_hidden.mean(dim=1) #(1, hidden_size)\n",
    "  return avg_pooled.squeeze().cpu() #removes batch dimension(hidden_size)\n",
    "\n",
    "\n",
    "#extract embeddings for all tweets\n",
    "bert_embeddings = []\n",
    "roberta_embeddings = []\n",
    "deberta_embeddings = []\n",
    "\n",
    "print(\"Extracting embeddings...\")\n",
    "\n",
    "for text in tqdm(df['text']):\n",
    "  bert_embed = get_embedding(text, bert_tokenizer, bert_model)\n",
    "  roberta_embed = get_embedding(text, roberta_tokenizer, roberta_model)\n",
    "  deberta_embed = get_embedding(text, deberta_tokenizer, deberta_model)\n",
    "\n",
    "  bert_embeddings.append(bert_embed)\n",
    "  roberta_embeddings.append(roberta_embed)\n",
    "  deberta_embeddings.append(deberta_embed)\n",
    "\n",
    "print(\"Embeddings extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "bert_tensor = torch.stack(bert_embeddings)\n",
    "roberta_tensor = torch.stack(roberta_embeddings)\n",
    "deberta_tensor = torch.stack(deberta_embeddings)\n",
    "\n",
    "# Ensure same hidden size for BER, Roberta, Deberta(768)\n",
    "assert bert_tensor.shape[1] == roberta_tensor.shape[1] == deberta_tensor.shape[1] == 768\n",
    "\n",
    "#ensemble\n",
    "stacked = torch.stack([bert_tensor, roberta_tensor, deberta_tensor], dim=2) #size : (num_samples, hidden_size, 3)\n",
    "\n",
    "#flattening\n",
    "fused_embeddings = stacked.mean(dim=2)\n",
    "\n",
    "#save\n",
    "torch.save(fused_embeddings, \"Feature_Matrix.pt\")\n",
    "torch.save(torch.tensor(df['label'].values), \"Labels.pt\")\n",
    "\n",
    "print(\"Embeddings extracted and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGoA07zy3ZMz"
   },
   "source": [
    "**Graph Construction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install networkx torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature matrix and labels\n",
    "feature_matrix = torch.load(\"/content/Feature_Matrix.pt\", weights_only=False)  # shape: (N, hidden_size)\n",
    "labels = torch.load(\"/content/Labels.pt\", weights_only=False)                  # shape: (N,)\n",
    "\n",
    "print(\"Feature matrix:\", feature_matrix.shape)\n",
    "print(\"Labels:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing subgraphs\n",
    "BATCH_SIZE = 500\n",
    "\n",
    "all_subgraphs = []\n",
    "num_samples = feature_matrix.shape[0]\n",
    "\n",
    "print(\"constructing subgraphs...\")\n",
    "\n",
    "for start_idx in tqdm(range(0, num_samples, BATCH_SIZE)):\n",
    "  end_idx = min(start_idx + BATCH_SIZE, num_samples)\n",
    "  batch_feats = feature_matrix[start_idx:end_idx]  # (C, 768)\n",
    "\n",
    "  adj_matrix = torch.mm(batch_feats, batch_feats.T)  # adjacency matrix(C, C)\n",
    "  adj_np = adj_matrix.numpy()\n",
    "\n",
    "  G = nx.Graph()\n",
    "  G.add_nodes_from(range(start_idx, end_idx)) #build networkx subgraph\n",
    "\n",
    "  for i in range(adj_np.shape[0]):\n",
    "    for j in range(i+1, adj_np.shape[1]):\n",
    "      weight = adj_np[i, j]\n",
    "      if weight > 0:\n",
    "        G.add_edge(start_idx + i, start_idx + j, weight = weight)\n",
    "\n",
    "  all_subgraphs.append(G)\n",
    "\n",
    "print(\"Subgraphs constructed.\")\n",
    "\n",
    "print(\"Merging subgraphs...\")\n",
    "full_graph = nx.compose_all(all_subgraphs) #merging subgraphs into a single graph\n",
    "print(\"Subgraphs merged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor(list(full_graph.edges), dtype=torch.long).t().contiguous() # converting to pytorch geometric format\n",
    "x = feature_matrix\n",
    "y = labels\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Save graph data\n",
    "torch.save(data, \"Graph_Data.pt\")\n",
    "\n",
    "print(\"✅ Graph construction complete.\")\n",
    "print(\"Graph stats:\")\n",
    "print(f\"- Nodes: {data.num_nodes}\")\n",
    "print(f\"- Edges: {data.num_edges}\")\n",
    "print(f\"- Feature dim: {data.num_node_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch-scatter\n",
    "!pip install torch-sparse\n",
    "!pip install scikit-learn\n",
    "#pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graph data\n",
    "data: Data = torch.load(\"/content/drive/MyDrive/Colab Notebooks/GAT/Graph_Data.pt\", weights_only=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masks(num_nodes, train_ratio=0.6, val_ratio=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_nodes)\n",
    "    train_size = int(train_ratio * num_nodes)\n",
    "    val_size = int(val_ratio * num_nodes)\n",
    "\n",
    "    train_idx = indices[:train_size]\n",
    "    val_idx = indices[train_size:train_size + val_size]\n",
    "    test_idx = indices[train_size + val_size:]\n",
    "\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "\n",
    "    return train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_mask, data.val_mask, data.test_mask = generate_masks(data.num_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define GAT model\n",
    "from torch.nn import Linear, Dropout\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1, dropout=0.5):\n",
    "        super(GAT, self).__init__()\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.gat2 = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout)\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT(in_channels=data.num_node_features, hidden_channels=128, out_channels=2, heads=4).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    logits = model(data.x, data.edge_index)\n",
    "    preds = logits.argmax(dim=1)\n",
    "\n",
    "    results = {}\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        mask = data[f'{split}_mask']\n",
    "        y_true = data.y[mask].cpu()\n",
    "        y_pred = preds[mask].cpu()\n",
    "        results[split] = {\n",
    "            'acc': accuracy_score(y_true, y_pred),\n",
    "            'f1': f1_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred),\n",
    "            'recall': recall_score(y_true, y_pred)\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 1001):  # 50 epochs\n",
    "    loss = train()\n",
    "    if epoch % 50 == 0:\n",
    "        metrics = evaluate()\n",
    "        print(f\"[Epoch {epoch:02d}] Loss: {loss:.4f} | \"\n",
    "              f\"Val Acc: {metrics['val']['acc']:.4f} | \"\n",
    "              f\"F1: {metrics['val']['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Final test evaluation\n",
    "final_metrics = evaluate()\n",
    "print(\"\\n✅ Final Test Metrics:\")\n",
    "for metric, value in final_metrics['test'].items():\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
